---
title: "6. Limitations and Recommendations"
author: "B233241"
date: "2025-01-07"
output:
  pdf_document:
    latex_engine: xelatex
    dev: cairo_pdf
knit: (function(inputFile, encoding) {
      rmarkdown::render(inputFile,
                        encoding = encoding,
                        knit_root_dir = rprojroot::find_rstudio_root_file(),
                        output_dir = file.path("../Output")
                        )})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(biostats101)

```

Word count: `r wordcountaddin::word_count(here::here("MSc_Thesis_Split", "Script", "_6_Limitations_Recommendations.Rmd"))`

# Limitations and Recommendations

## Limitations 

### Simulation Study {#lim-sim}

Key objectives of this study were to evaluate the precision and consistency of MR-Hevo causal estimation under differing sets of assumptions, including differing proportions of invalid genetic instruments. The random seed used happened to assign similar numbers (6 vs 7) of invalid instruments to both the 20% and 30% invalid instrument cases; relevant code was checked to ensure this was not an error in specification of model parameters. As this was noted after analysis had begun, it was decided not to re-run simulations to avoid implicit multiple comparisons in analysis. However, the resulting datasets generated arguably do not represent the true differences expected between cohorts with a 10% change in valid instrument proportion. In particular, Scenario 3 simulations may have been disproportionately affected by this phenomenon. \acr{InSIDE} assumption violation means each invalid instrument may introduce different proportions of noise and bias to the causal effect estimates generated. If the average noise introduced per instrument is greater than the average bias introduced per instrument, then adding a single extra invalid instrument may act to bias towards the null if its predominant effect is to reduce estimate precision; the addition of several extra instruments may be required for noise terms to average each other out and so for bias terms to predominate. This may explain why several trends in causal estimates, confidence intervals and causal report rates abruptly plateau around 20% under assumptions for Scenarios 1 and 2, and reverse under Scenario 3. Conclusions drawn from trends tracking the progression from 0% to 20% invalid instruments should be unaffected, but speculating on trends with $\ge$30% invalid instruments from these data alone would seem inadvisable. If desired, these simulation studies could be extended to progressively larger proportions of invalid instruments to investigate this possibility further.

This study had intended to exactly duplicate the simulation methodology of Bowden et al[@bowden_consistent_2016] to maximise comparability of results; however several barriers prevented this. As outlined in {#Methods}, the method of genotype generation, including frequencies of effect/non-effect alleles of simulated genetic instruments, was not reported by Bowden et al[@bowden_consistent_2016]. In addition, Bowden et al used 10,000 simulations per meta-analysis of each combination of Scenario assumptions and simulation parameters; this was rendered impractical due to the computationally intensive nature of MR-Hevo's resampling approach. On a mid-range home desktop computer `r #benchmarkme::get_sys_details()`, each meta-analysis took in the order of 8 hours to process $n = 1,000$ datasets; assuming $O(n)$ growth in processing time, this implies reproducing both tables would take `r ((8*2*24)/24/7) %>% round(., digits = 2)`of processing time, which was not practical with the time available. This was also realised too late in the project timeline to arrange for alternative computing capability, such as through the University of Edinburgh's compute cluster "Eddie"[@***ref***]. Although exact replication would have been desirable, using 1,000 simulated datasets per analysis appears to have been sufficient to generate comparable mean values of the parameter estimates to those reported by Bowden et al[@bowden_consistent_2016]. The main effect of increasing number of datasets per meta-analysis would be to improve precision of the mean estimates of each parameter (i.e. precision of mean causal effect estimates, mean \acr{SE}s and mean \acr{CI}s). As the spread of these parameter estimates is not reported in Tables \@ref(tab:no-causal-sim-summ-display) and \@ref(tab:causal-sim-summ-display), this change is not likely to have affected conclusions from this study.



### Re-Analysis of Published Data
```{r lim-cite-inline-vals}

max_FPRR_row <- no_causal_sim_summ_tib %>% 
  mutate(Pos_Rate_Diff_Abs = abs(Pos_Rate_Diff)) %>% 
  filter(Pos_Rate_Diff_Abs == max(Pos_Rate_Diff_Abs))

#max_FPRR_row

samp_size_mean_diff <- power.paired.prop(p1 = 0.0041, p2 = 0.051, conf.level = 0.95,power = 0.8,alternative = "two.sided")
samp_size_max_diff <- power.paired.prop(p1 = max_FPRR_row$Hevo_Pos_Rate, p2 = max_FPRR_row$WME_Pos_Rate, conf.level = 0.95,power = 0.8,alternative = "two.sided")

```

As noted in {#Results}, reproducibility of published findings from each re-analysed study was sub-optimal, despite using the same set of genetic instruments as reported in each study. The degree of divergence between published and re-analysed values was not anticipated, and therefore not accounted for in study design. It had been expected that \acr{WME} re-analysis would confirm published findings more closely, allowing comparison of MR-Hevo vs re-analysed to be a valid proxy for comparison of MR-Hevo vs published \acr{WME} results; this was only the case in four re-analysed studies. In this project, results of re-analysis were included irrespective of consistency of results with published data; however, any similar future work could employee exclusion criteria for studies whose estimates are not replicable within a specified error margin.

Even if all ten studies had given consistent estimates on re-analysis, this study would have been substantially under-powered to detect a true difference between methods. Using the mean difference in false-positive report rate abserved across all scenarion/parameter combinations in the simulation study (MR-Hevo = `r no_causal_Hevo_mean_pos_rate`% vs \acr{WME} = `r no_causal_WME_mean_pos_rate`%, \@ref(#results-sim-no-causal)), to detect a difference of this size with $\alpha = 0.05$ and $1 - \beta = 0.8$ would require a sample size of around `r samp_size_mean_diff$sample_size` studies to be re-analysed by both methods. If only 40% of studies were adequately reproducible for inclusion, the required sample size for re-analysis would increase to `r (samp_size_mean_diff$sample_size/4*10) %>% round(., 0)`. Even with the most extreme difference in false positive report rates observed (MR-Hevo = `r max_FPRR_row$Hevo_Pos_Rate * 100`% vs \acr{WME} = `r max_FPRR_row$WME_Pos_Rate * 100`%  $N =$ `r max_FPRR_row$N`, `r max_FPRR_row$Prop_Invalid * 100`% invalid instruments, Scenario 3; Table \@ref(tab:no-causal-sim-summ-display)), the required sample size would be `r samp_size_max_diff$sample_size`, or `r (samp_size_max_diff$sample_size/4*10) %>% round(., 0)` if only 40% of studies were adequately reproducible for inclusion. As such, it is not possible to conclude that MR-Hevo methods might not change conclusions in a substantial number of studies; as stated in {#Methods}, the intent of this project was to help deliniate the upper bound of the potential effect of MR-Hevo on the field of \acr{MR}.

## Recommendations

