# Limitations and Recommendations

```{r setup6, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(biostats101)

```


## Limitations 

### Simulation Study {#lim-sim}

Key objectives of this study were to evaluate the accuracy, precision and consistency of MR-Hevo causal estimation under differing sets of assumptions, including differing proportions of invalid genetic instruments. The random seed used happened to assign similar numbers (6 vs 7) of invalid instruments to both the 20% and 30% invalid instrument cases; relevant code was checked to ensure this was not an error in specification of model parameters. As this was noted after analysis had begun, it was decided not to re-run simulations to avoid implicit multiple comparisons in analysis. However, the resulting datasets generated arguably do not represent the true differences expected between cohorts with a 10% change in valid instrument proportion. In particular, Scenario 3 simulations may have been disproportionately affected by this phenomenon. \acr{InSIDE} assumption violation means each invalid instrument may introduce different proportions of noise and bias to the causal effect estimates generated. If the average noise introduced per instrument is greater than the average bias introduced per instrument, then adding a single extra invalid instrument may act to bias towards the null if its predominant effect is to reduce estimate precision; the addition of several extra instruments may be required for noise terms to average each other out and so for bias terms to predominate. This may explain why several trends in causal estimates, confidence intervals and causal report rates abruptly plateau around 20% under assumptions for Scenarios 1 and 2, and reverse under Scenario 3. Conclusions drawn from trends tracking the progression from 0% to 20% invalid instruments should be unaffected, but speculating on trends with $\ge$ 30% invalid instruments from these data alone would seem inadvisable. If desired, these simulation studies could be extended to progressively larger proportions of invalid instruments to investigate this possibility further.

This study had intended to exactly duplicate the simulation methodology of Bowden et al[@bowden_consistent_2016] to maximise comparability of results; however several barriers prevented this. As outlined in [Methods], the process of genotype generation, including frequencies of effect/non-effect alleles of simulated genetic instruments, was not reported by Bowden et al[@bowden_consistent_2016]. In addition, Bowden et al used 10,000 simulations per meta-analysis of each combination of Scenario assumptions and simulation parameters; this was rendered impractical due to the computationally intensive nature of MR-Hevo's resampling approach. On a mid-range home desktop computer (specification found in Appendix \@ref(Session-Information)), each meta-analysis took in the order of 8 hours to process $n = 1,000$ datasets; testing indicated that the vast majority of processing time was used by MR-Hevo, rather than \acr{WME}. Assuming $O(n)$ growth in processing time, this implies reproducing both tables would take `r ((8*2*24)/24/7) %>% round(., digits = 2)` weeks of processing time, which was not practical with the time available. This was also realised too late in the project timeline to arrange for alternative computing capability, such as through the University of Edinburgh's compute cluster "Eddie"[@noauthor_eddie_nodate]. Although exact replication would have been desirable, using 1,000 simulated datasets per analysis appears to have been sufficient to generate comparable mean values of the parameter estimates to those reported by Bowden et al[@bowden_consistent_2016]. The main effect of increasing number of datasets per meta-analysis would be to improve precision of the mean estimates of each parameter (i.e. precision of mean causal effect estimates, mean \acr{SE}s and mean \acr{CI}s). As the spread of these parameter estimates is not reported in Tables \@ref(tab:no-causal-sim-summ-display) and \@ref(tab:causal-sim-summ-display), this change is not likely to have affected conclusions from this study.

Finally, it was noted that parameters used during simulation were not representative of values observed in the ten highly-cited studies used for the re-analysis section of this project. Both number of genetic instruments and number of participants were at least an order of magnitude lower in simulations versus re-analysed studies. Additionally, due to the nature of two-sample \acr{MR} studies, numbers of participants in exposure versus outcome cohorts varied substantially in several real-world studies, whereas the simulation code only allowed equally sized cohorts. As mentioned in [Methods], effect allele frequency was simulated around 50% in line with Bowden et al[@bowden_consistent_2016]; where re-analysed studies reported effect allele frequencies, these typically varied from around 10-90% across the range of genetic instruments included. The effects of varying any of these parameters, either alone or in combination, could plausibly affect the sensitivity and specificity of any \acr{MR} causal effect estimation method. It may be that these parameter values reflect the majority of \acr{MR} literature better than these highly-cited studies, which by their nature would be expected to include more data than is typical for the field. However, given that these parameter values are taken from the original \acr{WME} exposition[@bowden_consistent_2016], it is possible that they were originally chosen as a "best case scenario", intended to represent \acr{WME} performance in the most favourable possible light. If this were the case, then \acr{WME} could paradoxically struggle more with the increasingly comprehensive datasets now commonly used for \acr{MR} studies. Extending this project using simulation parameter values more representative of real-world data would therefore seem warranted before drawing definitive conclusions regarding relative performance of both methods.



### Re-Analysis of Published Data {#lim-cite}
```{r lim-cite-inline-vals}

max_FPRR_row <- no_causal_sim_summ_tib %>% 
  mutate(Pos_Rate_Diff_Abs = abs(Pos_Rate_Diff)) %>% 
  filter(Pos_Rate_Diff_Abs == max(Pos_Rate_Diff_Abs))

#max_FPRR_row

samp_size_mean_diff <- power.paired.prop(p1 = 0.0041, p2 = 0.051, conf.level = 0.95,power = 0.8,alternative = "two.sided")
samp_size_max_diff <- power.paired.prop(p1 = max_FPRR_row$Hevo_Pos_Rate, p2 = max_FPRR_row$WME_Pos_Rate, conf.level = 0.95,power = 0.8,alternative = "two.sided")

```

As noted in [Results], reproducibility of published findings from each re-analysed study was sub-optimal, despite using the same set of genetic instruments as reported in each study. The degree of divergence between published and re-analysed values was not anticipated, and therefore not accounted for in study design. It had been expected that \acr{WME} re-analysis would confirm published findings more closely, allowing comparison of MR-Hevo vs re-analysed to be a valid proxy for comparison of MR-Hevo vs published \acr{WME} results; this was only the case in six re-analysed studies. In this project, results of re-analysis were included irrespective of consistency of results with published data; however, any similar future work could employee exclusion criteria for studies whose estimates are not replicable within a specified error margin.

Even if all ten studies had given consistent estimates on re-analysis, this study would have been substantially under-powered to detect a true difference between methods. Using the mean difference in false-positive report rate abserved across all scenario/parameter combinations in the simulation study (MR-Hevo = `r no_causal_Hevo_mean_pos_rate`% vs \acr{WME} = `r no_causal_WME_mean_pos_rate`%, Section \@ref(results-sim-no-causal)), to detect a difference of this size with $\alpha = 0.05$ and $1 - \beta = 0.8$ would require a sample size of around `r samp_size_mean_diff$sample_size` studies to be re-analysed by both methods. If only 40% of studies were adequately reproducible for inclusion, the required sample size for re-analysis would increase to `r (samp_size_mean_diff$sample_size/4*10) %>% round(., 0)`. Even with the most extreme difference in false positive report rates observed (MR-Hevo = `r max_FPRR_row$Hevo_Pos_Rate * 100`% vs \acr{WME} = `r max_FPRR_row$WME_Pos_Rate * 100`%;  $N =$ `r max_FPRR_row$N %>% format(scientific = FALSE)`, `r max_FPRR_row$Prop_Invalid * 100`% invalid instruments, Scenario 3; Table \@ref(tab:no-causal-sim-summ-display)), the required sample size would be `r samp_size_max_diff$sample_size`, or `r (samp_size_max_diff$sample_size/4*10) %>% round(., 0)` if only 40% of studies were adequately reproducible for inclusion. As such, it is not possible to conclude that MR-Hevo methods might not change conclusions in a substantial number of studies; as stated in [Methods], the intent of this project was to help delineate the upper bound of the potential effect of MR-Hevo on the field of \acr{MR}.

## Recommendations

The results of this project do not suggest the need to disregard every \acr{MR}-derived causal association identified using \acr{WME} methodology. However, the results would be compatible with a substantial absolute number of studies in the literature which may falsely report a causal effect due to use of \acr{WME}. Furthermore, causal reports from \acr{MR} studies may separately be untrustworthy by virtue of not being reproducible from data and methodology reported; the extent to which this affects the \acr{MR} evidence base is not known. If MR-Hevo were used in place of \acr{WME} to identify potential causality in future \acr{MR} studies, this would be expected to lower the false-positive report rate by up to ~25% - though with a potential loss of statistical power of ~10-20%. Given the field-wide concerns regarding high false-positive rates, this may be a reasonable compromise.

Following this project, the recommendations below are offered:

- Further research is required to estimate the proportion of \acr{MR} literature whose results and conclusions are not reproducible from the data and methods presented. Such work would ideally investigate potential causes of such discrepancies (e.g. the contribution of rounding errors in summary results) so that guidance can be developed to prevent further non-reproducible studies being created. A less pressing research suggestion would be to evaluate the effect of varying simulation parameter values (e.g. number of participants, number of genetic insruments and effect allele frequency) on performance of \acr{MR} causal estimation methods.

- Before taking any significant action on the results of any \acr{MR} study reporting causality using any methodology, attempts should be made to reproduce key findings from reported data and methods. Where this is not possible (e.g. due to data availability restrictions), consideration should be given as to whether said significant actions would still be taken if effect estimates and/or their \acr{CI}s were to alter by a plausible margin of around $\beta = \pm 0.1$

- For interpretation of existing \acr{MR} studies relying on \acr{WME} causal effect estimation, re-analysing using MR-Hevo methods is unlikely to alter the magnitude or direction of estimated causal effect. Where a \acr{WME} \acr{MR} study reports no evidence of causality, MR-Hevo re-analysis is unlikely to overturn this conclusion. MR-Hevo is, however, more conservative than \acr{WME} when generating \acr{CI}s; it is therefore likely to change overall interpretation in a significant minority of cases reported as supporting a truly causal exposure-outcome association. Re-analysis of \acr{WME} \acr{MR} studies may therefore be warranted as a sensitivity analysis of \acr{WME} \acr{MR} studies reporting causality, either where significant action is planned on the strength of the results, or where the validity of the result is questioned.

- For future \acr{MR} studies looking to establish potential causal links between exposures and outcomes, use of MR-Hevo causal estimation is expected to produce a lower false-positive causal report rate than \acr{WME} methods. The main disadvantages are a) a corresponding loss in power, which seems a worthwhile trade-off given high false positive rates in the \acr{MR} literature more broadly; and b) the extra compute required, though for most applications this difference will be trivial.